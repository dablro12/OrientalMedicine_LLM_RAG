import os
import streamlit as st
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable
from langchain_openai import ChatOpenAI
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

import sys 
from pathlib import Path
sys.path.append(str(Path(os.getcwd()).parent))
sys.path.append('modules')
from embedding import get_embeddings, binary_embed_file, multi_embed_files
# â­ï¸ Embedding ì„¤ì •
# USE_BGE_EMBEDDING = True ë¡œ ì„¤ì •ì‹œ HuggingFace BAAI/bge-m3 ìž„ë² ë”© ì‚¬ìš© (2.7GB ë‹¤ìš´ë¡œë“œ ì‹œê°„ ê±¸ë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤)
# USE_BGE_EMBEDDING = False ë¡œ ì„¤ì •ì‹œ OpenAIEmbeddings ì‚¬ìš© (OPENAI_API_KEY ìž…ë ¥ í•„ìš”. ê³¼ê¸ˆ)
USE_BGE_EMBEDDING = True

# â­ï¸ LangServe ëª¨ë¸ ì„¤ì •(EndPoint)
# 1) REMOTE ì ‘ì†: ë³¸ì¸ì˜ REMOTE LANGSERVE ì£¼ì†Œ ìž…ë ¥
# LANGSERVE_ENDPOINT = "https://warm-newly-stag.ngrok-free.app/llm/" # ngrok + lanserve
LANGSERVE_ENDPOINT = "http://localhost:8000/llm/"

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± @Mineru
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# í”„ë¡¬í”„íŠ¸ë¥¼ ìžìœ ë¡­ê²Œ ìˆ˜ì •í•´ ë³´ì„¸ìš”!
RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ì •í™•ížˆ ë‹µë³€í•˜ëŠ” AI ìž…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ì¶œì²˜ë¥¼ í¬í•¨í•˜ì—¬ ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”. 
Question: {question} 
Context: {context} 
Answer:"""

st.set_page_config(page_title="main1", page_icon="ðŸ’¬")
st.title("SNUH-MEDISC Eiden Local LLM Dev")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ê³ ê°ë‹˜ì˜ ë³´í—˜ ì„œë¥˜ë¥¼ ì¤€ë¹„í•´ë“œë¦¬ëŠ” AI ì–´ì‹œìŠ¤í„´ìŠ¤ìž…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))

def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)


def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)

with st.sidebar: # ì—¬ëŸ¬ ì²´í¬ë°•ìŠ¤ë¥¼ ì¶”ê°€í•  ìˆ˜ ìžˆëŠ” ê¸°ëŠ¥
    st.write("")
    options = st.multiselect(
        "ðŸ”§ ë³´í—˜ì‚¬ ì„¤ì •(ë³µìˆ˜ì„¤ì •ê°€ëŠ¥)",
        [
            "AIAìƒëª…","MGì†í•´ë³´í—˜","ë†í˜‘ì†í•´ë³´í—˜","ë™ì–‘ìƒëª…","ë¼ì´ë‚˜ì†ë³´","ë¡¯ë°ì†í•´ë³´í—˜","ì‚¼ì„±í™”ìž¬ ë‹¤ì´ë ‰íŠ¸","ìš°ì²´êµ­ë³´í—˜","í•˜ë‚˜ìƒëª…","í•œí™”ìƒëª…","KBë¼ì´í”„","ì‚¼ì„±í™”ìž¬","ì‚¼ì„±ìƒëª…"
        ]
    ) # -> list 
    
    # 
    file = st.file_uploader(
        "ì°¸ê³  íŒŒì¼ ì—…ë¡œë“œ(ì—¬ëŸ¬ íŒŒì¼ ì—…ë¡œë“œê°€ëŠ¥)",
        type=["pdf", "txt", "docx", "csv", "xlsx", "html", "json"],
        accept_multiple_files=True
    )
print(file)
if len(file) > 0:
    if len(file) == 1:
        retriever = binary_embed_file(file[0])
    elif len(file) > 1:
        retriever = multi_embed_file(file)
else:
    file = None

print_history()

if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        # ngrok remote ì£¼ì†Œ ì„¤ì •
        ollama = RemoteRunnable(LANGSERVE_ENDPOINT) #<-- GPU ì„œë²„ì—ì„œ ì‹¤í–‰ì„ ìœ„í•´ ngrok + langserve ì´ìš©

        chat_container = st.empty()
        if file is not None:
            prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            rag_chain = (
                {
                    "context": retriever | format_docs,
                    "question": RunnablePassthrough(),
                }
                | prompt
                | ollama
                | StrOutputParser()
            )
            # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ìž…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
            answer = rag_chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))
        else: # íŒŒì¼ê³¼ ë³´í—˜ì‚¬ ì„¤ì •ì´ ì—†ëŠ” ê²½ìš° ì „ì²´ì— ëŒ€í•´ì„œ ì²˜ë¦¬
            print(f"file is none")
            prompt = ChatPromptTemplate.from_template(
                "ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”:\n{input}"
            )
            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            chain = prompt | ollama | StrOutputParser()

            answer = chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))
