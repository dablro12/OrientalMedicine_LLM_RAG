import os
import streamlit as st
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable
from langchain_openai import ChatOpenAI
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

import sys 
from pathlib import Path

class SNUHMEDISCApp:
    def __init__(self, prompt, examples):
        self.setup_paths()
        self.setup_page()
        self.initialize_session_state()
        self.LANGSERVE_ENDPOINT = "http://localhost:8000/llm/"
        self.prompt = prompt 
        self.examples = examples
        self.RAG_PROMPT_TEMPLATE = self.format_prompt(self.prompt, self.select_examples(examples))
        self.NO_RAG_PROMPT_TEMPLATE = self.no_retreival_prompt(self.prompt, self.select_examples(examples))
        self.USE_BGE_EMBEDDING = True
        self.setup_sidebar()
        self.print_history()

    def setup_paths(self):
        """ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì„¤ì • """
        sys.path.append(str(Path(os.getcwd()).parent))
        sys.path.append('modules')
        from embedding import binary_embed_file, multi_embed_files, url_embed_file
        from matching import matching_insurance
        from prompt import select_examples, format_prompt, No_retrieval_format_prompt, RAG_chain, RAG_simple_chain, qa_chain
        # from reference_load import reference_load
        self.binary_embed_file = binary_embed_file
        self.multi_embed_file = multi_embed_files
        self.url_embed_file = url_embed_file
        self.matching_insurance = matching_insurance
        self.select_examples = select_examples
        self.format_prompt = format_prompt
        self.no_retreival_prompt = No_retrieval_format_prompt
        self.RAG_chain = RAG_chain
        self.RAG_simple_chain = RAG_simple_chain
        self.qa_chain = qa_chain
        # self.reference_load = reference_load

    def setup_page(self):
        st.set_page_config(page_title="Local LLM with RAG", page_icon="ğŸ’¬")
        st.title("Local Koream Medicine LLM with RAG")

    def initialize_session_state(self):
        if "messages" not in st.session_state:
            st.session_state["messages"] = [
                ChatMessage(role="assistant", content="í•œì˜í•™ ë°•ì‚¬ AI ì–´ì‹œìŠ¤í„´ìŠ¤ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
            ]

    def setup_sidebar(self):
        # sidebar ì„¤ì •
        with st.sidebar:
            self.file, self.user_urls = None, None
            self.retrieval, self.db = None, None
            
            st.write("")
            # self.options = st.multiselect(
            #     "ğŸ”§ ë³´í—˜ì‚¬ ì„¤ì •(ë³µìˆ˜ì„¤ì •ê°€ëŠ¥)",
            #     ["AIAìƒëª…", "MGì†í•´ë³´í—˜", "ë†í˜‘ì†í•´ë³´í—˜", "ë™ì–‘ìƒëª…", "ë¼ì´ë‚˜ì†ë³´", "ë¡¯ë°ì†í•´ë³´í—˜", "ì‚¼ì„±í™”ì¬ ë‹¤ì´ë ‰íŠ¸", "ìš°ì²´êµ­ë³´í—˜", "í•˜ë‚˜ìƒëª…", "í•œí™”ìƒëª…", "KBë¼ì´í”„", "ì‚¼ì„±í™”ì¬", "ì‚¼ì„±ìƒëª…"]
            # )
            
            # self.user_urls = self.matching_insurance(
            #     user_insurance_li=self.options,
            #     url_json = 'data/insuarance/insuarance_url.json')
            
            self.file = st.file_uploader(
                "ğŸ”§ ì°¸ê³  íŒŒì¼ ì—…ë¡œë“œ(ì—¬ëŸ¬ íŒŒì¼ ì—…ë¡œë“œê°€ëŠ¥)",
                type=["pdf", "txt", "docx", "csv", "xlsx", "html", "json"],
                accept_multiple_files=True
            )
            # 1. URLS 
            # 2. URLS + Files
            # 3. Files
            # 4. None
            if self.file:
                if len(self.file) == 1:
                    self.file = self.file[0]
                    self.retrieval, self.db = self.binary_embed_file(self.file, self.user_urls)
                else:
                    self.retrieval, self.db = self.multi_embed_file(self.file, self.user_urls)
            elif self.user_urls:
                self.retrieval, self.db = self.url_embed_file(self.user_urls)
            else:
                self.retrieval, self.db = None, None
                
            
            # if self.user_urls and self.file:  # Check if both user_urls and file are not empty or None
            #     if len(self.file) == 1: # ë³´í—˜ì‚¬ ì„¤ì • O / ë‹¨ì¼ íŒŒì¼
            #         self.retrieval, self.db = self.binary_embed_file(self.file[0], self.user_urls)
            #     else: # ë³´í—˜ì‚¬ ì„¤ì • O / ë‹¤ì¤‘ íŒŒì¼
            #         self.retrieval, self.db = self.multi_embed_file(self.file, self.user_urls)
            # elif self.user_urls and not self.file: # ë³´í—˜ì‚¬ ì„¤ì • O / íŒŒì¼ X
            #     self.retrieval, self.db = self.url_embed_file(self.user_urls)
            # elif not self.user_urls and self.file: # ë³´í—˜ì‚¬ ì„¤ì • X / íŒŒì¼ O
            #     self.retrieval, self.db = self.binary_embed_file(self.file[0], self.user_urls)
            # else:
            #     self.retrieval, self.db = None, None

    def print_history(self):
        for msg in st.session_state.messages:
            st.chat_message(msg.role).write(msg.content)

    def add_history(self, role, content):
        st.session_state.messages.append(ChatMessage(role=role, content=content))

    def format_docs(self, docs):
        return "\n\n".join(doc.page_content for doc in docs)

    def handle_user_input(self, user_input):
        """ Main Operation Function """ 
        self.add_history("user", user_input)
        st.chat_message("user").write(user_input)
        with st.chat_message("assistant"):
            ollama = RemoteRunnable(self.LANGSERVE_ENDPOINT)
            chat_container = st.empty()
            if self.retrieval is not None: # Retrieval
                print("RAG Chain")
                prompt = ChatPromptTemplate.from_template(self.RAG_PROMPT_TEMPLATE)
                rag_chain = self.RAG_chain(retrieval=self.retrieval, format_docs=self.format_docs, prompt=prompt, ollama=ollama, StrOutputParser=StrOutputParser)
                answer = rag_chain.stream(user_input)
                chunks = []
                for chunk in answer:
                    chunks.append(chunk)
                    chat_container.markdown("".join(chunks))
                self.add_history("ai", "".join(chunks))
            else: # No Retrieval
                print("No RAG Chain")
                prompt = ChatPromptTemplate.from_template(
                    self.NO_RAG_PROMPT_TEMPLATE
                )
                
                chain = prompt | ollama | StrOutputParser()
                answer = chain.stream(user_input)
                chunks = []
                for chunk in answer:
                    chunks.append(chunk)
                    chat_container.markdown("".join(chunks))
                self.add_history("ai", "".join(chunks))

            #############################################################################################################################
            
    def display_url_references(self, source_documents, k):
        st.markdown("## ì°¸ê³  ìë£Œ")
        for idx, reference in enumerate(source_documents):
            if idx >= k:
                break
            st.markdown(f"**ì¶œì²˜:** [{reference.metadata['source']}]({reference.metadata['source']})")
            self.add_history("assistant", f"ì¶œì²˜: [{reference.metadata['source']}]({reference.metadata['source']})")
    
    ##############################################################################################################################
    
    def run(self):
        if user_input := st.chat_input():
            self.handle_user_input(user_input)

if __name__ == "__main__":
    # app = SNUHMEDISCApp()
    # app.run()
    # ì„¸ì…˜ ì´ˆê¸°í™”ì„¸íŒ…
    if "first_run" not in st.session_state:
        st.session_state.first_run = True
        # Register session state reset callback
        st.session_state["reset"] = False
    
    INSTRUMENT = "ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ì •í™•í•œ ëŒ€ë‹µì„ í•´ì£¼ëŠ” ì„ ìƒë‹˜ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ì¶œì²˜ë¥¼ í¬í•¨í•˜ì—¬ ë‹µí•˜ì„¸ìš”. <Example Answer>ì˜ í˜•ì‹ì— ë§ì¶°ì„œ ëŒ€ë‹µí•˜ì„¸ìš”. ë¬¸ì„œì— ëŒ€í•œ ë‚´ìš©ì´ ì•„ë‹Œ ê²½ìš° [ë‚´ìš©ì— í¬í•¨ëœ ëŒ€ë‹µì„ í•´ì£¼ì„¸ìš”]ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
    #Few-Shot
    EXAMPLE = [
        {"Question": "AIAë³´í—˜ì—ì„œ ì‹¤ì†ì˜ë£Œ ë³´í—˜ ì²­êµ¬ë¥¼ ìœ„í•´ í•„ìš”í•œ ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", "Answer": "[ì§„ë£Œë¹„ê³„ì‚°ì˜ìˆ˜ì¦ ë° ì§„ë£Œë¹„ ì„¸ë¶€ë‚´ì—­ì„œ]ì™€ [ì…í‡´ì›í™•ì¸ì„œ, ì§„ë‹¨ì„œ ì¤‘ íƒ 1]ê°€ í•„ìš”í•©ë‹ˆë‹¤."},
        {"Question": "í•˜ë‚˜ ìƒëª…ë³´í—˜ì—ì„œ ì…ì›ì‹œ ê³µí†µìœ¼ë¡œ ë‚´ì–´ì•¼í•  ì„œë¥˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”", "Answer": "[ì…í‡´ì›í™•ì¸ì„œ]ì™€ [ì§„ë‹¨ì„œ]ê°€ í•„ìš”í•©ë‹ˆë‹¤."},
        {"Question": "ë¼ì´ë‚˜ ìƒëª…ë³´í—˜ì—ì„œ ì—¬í–‰ìë³´í—˜ ì‚¬ê³ ì—ì„œ ê³µí†µì„œë¥˜ ì¤‘ ê¸°ë³¸ êµ¬ë¹„í•´ì•¼í•  ì„œë¥˜?", "Answer": "[ë³´í—˜ê¸ˆ ì²­êµ¬ì„œ]ì™€ [ì—¬ê¶Œì‚¬ë³¸ ë° ì—¬í–‰ì¼ì •í‘œ]ì™€ [ì²­êµ¬ì¸ ì‹ ë¶„ì¦ ì‚¬ë³¸]ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
    ]

    app = SNUHMEDISCApp(prompt=INSTRUMENT, examples=EXAMPLE)
    app.run()

    # If the reset flag is set, reset the session state
    if st.session_state["reset"]:
        app.reset_session_state()
        st.session_state["reset"] = False
